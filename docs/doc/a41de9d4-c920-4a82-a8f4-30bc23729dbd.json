{
    "summary": "The code installs libraries, loads the CLIP model, and processes a dataset before performing zero-shot classification and calculating top-1/top-5 accuracy on ImageNet dataset.",
    "details": [
        {
            "comment": "This code installs required libraries, loads OpenAI's CLIP model, and retrieves Imagenet classes and templates. It also imports the ImageNetV2Dataset from a specific repository.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Prompt_Engineering_for_ImageNet.py\":0-35",
            "content": "#! pip install ftfy regex tqdm\n#! pip install git+https://github.com/openai/CLIP.git\nimport numpy as np\nimport torch\nimport clip\nfrom tqdm.notebook import tqdm\nfrom pkg_resources import packaging\nprint(\"Torch version:\", torch.__version__)\nclip.available_models()\nmodel, preprocess = clip.load(\"ViT-B/32\")\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)\nimport json\nimagenet_data = json.loads(open(\"imagenet_data.json\",\"r\").read())\nimagenet_classes = imagenet_data['imagenet_classes']\nimagenet_templates = imagenet_data['imagenet_templates']\nprint(f\"{len(imagenet_classes)} classes, {len(imagenet_templates)} templates\")\n# execute:\n# ! pip install git+https://github.com/modestyachts/ImageNetV2_pytorch\nfrom imagenetv2_pytorch import ImageNetV2Dataset"
        },
        {
            "comment": "Code snippet performs zero-shot classification for the ImageNet dataset. It generates embeddings for given class names using text templates, averages them and stores in zeroshot_weights. The accuracy function calculates accuracy based on the output and target values.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Prompt_Engineering_for_ImageNet.py\":37-58",
            "content": "images = ImageNetV2Dataset(transform=preprocess)\nloader = torch.utils.data.DataLoader(images, batch_size=32, num_workers=2)\ndef zeroshot_classifier(classnames, templates):\n    with torch.no_grad():\n        zeroshot_weights = []\n        for classname in tqdm(classnames):\n            texts = [template.format(classname) for template in templates] #format with class\n            texts = clip.tokenize(texts).cuda() #tokenize\n            class_embeddings = model.encode_text(texts) #embed with text encoder\n            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n            class_embedding = class_embeddings.mean(dim=0)\n            class_embedding /= class_embedding.norm()\n            zeroshot_weights.append(class_embedding)\n        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n    return zeroshot_weights\nzeroshot_weights = zeroshot_classifier(imagenet_classes, imagenet_templates)\ndef accuracy(output, target, topk=(1,)):\n    pred = output.topk(max(topk), 1, True, True)[1].t()"
        },
        {
            "comment": "The code calculates the top-1 and top-5 accuracy of a model's predictions on ImageNet dataset. It computes the accuracy by comparing predicted probabilities with ground truth labels, averages them over all images in the batch, and prints the results.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Prompt_Engineering_for_ImageNet.py\":59-83",
            "content": "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\nwith torch.no_grad():\n    top1, top5, n = 0., 0., 0.\n    for i, (images, target) in enumerate(tqdm(loader)):\n        images = images.cuda()\n        target = target.cuda()\n        # predict\n        image_features = model.encode_image(images)\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        logits = 100. * image_features @ zeroshot_weights\n        # measure accuracy\n        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n        top1 += acc1\n        top5 += acc5\n        n += images.size(0)\ntop1 = (top1 / n) * 100\ntop5 = (top5 / n) * 100 \nprint(f\"Top-1 accuracy: {top1:.2f}\")\nprint(f\"Top-5 accuracy: {top5:.2f}\")"
        }
    ]
}