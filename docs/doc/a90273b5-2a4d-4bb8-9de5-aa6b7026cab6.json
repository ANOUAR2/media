{
    "summary": "The code imports libraries, prepares CLIP model and image datasets, calculates text-image similarity using CIFAR-100 dataset, and visualizes the relationship in a heatmap.",
    "details": [
        {
            "comment": "The code imports necessary libraries, checks the installed versions of PyTorch and CLIP, loads a pre-trained CLIP model with specified parameters, and defines some variables including image resolution, context length, and vocabulary size. It also displays the total number of model parameters and shows how to tokenize text using CLIP's tokenizer. The code then imports necessary libraries for image processing and visualization like skimage, IPython.display, matplotlib.pyplot, PIL, numpy, and torch. Finally, it defines a dictionary with image names as keys and their corresponding descriptions as values.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Interacting_with_CLIP.py\":0-45",
            "content": "#! pip install ftfy regex tqdm\n#! pip install git+https://github.com/openai/CLIP.git\nimport numpy as np\nimport torch\nfrom pkg_resources import packaging\nprint(\"Torch version:\", torch.__version__)\nimport clip\nclip.available_models()\nmodel, preprocess = clip.load(\"ViT-B/32\")\nmodel.cuda().eval()\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)\npreprocess\nclip.tokenize(\"Hello World!\")\nimport os\nimport skimage\nimport IPython.display\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom collections import OrderedDict\nimport torch\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n# images in skimage to use and their textual descriptions\ndescriptions = {\n    \"page\": \"a page of text about segmentation\",\n    \"chelsea\": \"a facial photo of a tabby cat\","
        },
        {
            "comment": "This code is preparing a dataset of images and corresponding descriptions for CLIP. It reads image files from the specified directory, selects relevant images based on provided descriptions, preprocesses them, and stores in lists. The images are then displayed as a grid with titles showing their names and descriptions. Finally, the preprocessed images are converted to torch tensor for use with CLIP.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Interacting_with_CLIP.py\":46-79",
            "content": "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n    \"rocket\": \"a rocket standing on a launchpad\",\n    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n    \"camera\": \"a person looking at a camera on a tripod\",\n    \"horse\": \"a black-and-white silhouette of a horse\", \n    \"coffee\": \"a cup of coffee on a saucer\"\n}\noriginal_images = []\nimages = []\ntexts = []\nplt.figure(figsize=(16, 5))\nfor filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n    name = os.path.splitext(filename)[0]\n    if name not in descriptions:\n        continue\n    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n    plt.subplot(2, 4, len(images) + 1)\n    plt.imshow(image)\n    plt.title(f\"{filename}\\n{descriptions[name]}\")\n    plt.xticks([])\n    plt.yticks([])\n    original_images.append(image)\n    images.append(preprocess(image))\n    texts.append(descriptions[name])\nplt.tight_layout()\nimage_input = torch.tensor(np.stack(images)).cuda()"
        },
        {
            "comment": "Code chunk normalizes text and image features, calculates cosine similarity between them, and plots a heatmap to visualize the relationship.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Interacting_with_CLIP.py\":80-109",
            "content": "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\nwith torch.no_grad():\n    image_features = model.encode_image(image_input).float()\n    text_features = model.encode_text(text_tokens).float()\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\ncount = len(descriptions)\nplt.figure(figsize=(20, 14))\nplt.imshow(similarity, vmin=0.1, vmax=0.3)\n# plt.colorbar()\nplt.yticks(range(count), texts, fontsize=18)\nplt.xticks([])\nfor i, image in enumerate(original_images):\n    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\nfor x in range(similarity.shape[1]):\n    for y in range(similarity.shape[0]):\n        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\nfor side in [\"left\", \"top\", \"right\", \"bottom\"]:\n  plt.gca().spines[side].set_visible(False)\nplt.xlim([-0.5, count - 0.5])\nplt.ylim([count + 0.5, -2])\nplt.title(\"Cosine similarity between text and image features\", size=20)"
        },
        {
            "comment": "This code is loading the CIFAR-100 dataset, extracting image features and text descriptions from it, then calculating the similarity between image features and text features. The results are displayed in a visualization showing the top 5 most probable labels for each image.",
            "location": "\"/media/root/Toshiba XG3/works/CLIP/docs/src/notebooks/Interacting_with_CLIP.py\":111-142",
            "content": "from torchvision.datasets import CIFAR100\ncifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)\ntext_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\ntext_tokens = clip.tokenize(text_descriptions).cuda()\nwith torch.no_grad():\n    text_features = model.encode_text(text_tokens).float()\n    text_features /= text_features.norm(dim=-1, keepdim=True)\ntext_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\ntop_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\nplt.figure(figsize=(16, 16))\nfor i, image in enumerate(original_images):\n    plt.subplot(4, 4, 2 * i + 1)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.subplot(4, 4, 2 * i + 2)\n    y = np.arange(top_probs.shape[-1])\n    plt.grid()\n    plt.barh(y, top_probs[i])\n    plt.gca().invert_yaxis()\n    plt.gca().set_axisbelow(True)\n    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n    plt.xlabel(\"probability\")\nplt.subplots_adjust(wspace=0.5)\nplt.show()"
        }
    ]
}